TODO:

[] Incorporate landfire queries:

Wrokflow brainstorm
    duet_run = import_duet(directory, nx ny)
    grass_density_sb40_targets = assign_targets_from_sb40(query = LandfireQuery, fuel_type = "grass", parameter = "density", method="maxmin")
    grass_density_sb40 = set_fuel_parameter(parameter = "density", grass = grass_density_sb40_targets)
    calibrate(duet_run, [grass_density_sb40, grass_moisture_constant])


# import geojson
# import landfire
# from landfire.geospatial import get_bbox_from_polygon
# import zipfile
# import rasterio as rio
# import rasterio.mask
# from rasterio.enums import Resampling
# import pandas as pd
# import re
#     def calibrate_with_sb40(self, fuel_type: str | list) -> None:
#         self._validate_inputs(fuel_type)
#         print("Querying LandFire...\n")
#         # Query Landfire and return array of SB40 keys
#         sb40_arr = self._query_landfire()
#         # Import SB40 FBFM parameters table
#         sb40_params_path = DATA_PATH / "sb40_parameters.csv"
#         sb40_params = pd.read_csv(sb40_params_path)
#         # Generate dict of fastfuels bulk density values and apply to Landfire query
#         sb40_dict = self._get_sb40_fuel_params(sb40_params)
#         sb40_ftype, sb40_rhof = self._get_sb40_arrays(sb40_arr, sb40_dict)
#         if isinstance(fuel_type, str):
#             fuel_type = [fuel_type]
#         calibrated = {}
#         for f in fuel_type:
#             if f == "grass":
#                 if 1 in sb40_ftype:
#                     max_val = np.max(sb40_rhof[sb40_ftype == 1])
#                     grass_arr = sb40_rhof[sb40_ftype == 1]
#                     min_val = np.min(grass_arr[grass_arr > 0])
#                 else:
#                     print(
#                         "WARNING: grass fuel not present in sb40. Continuing with litter only."
#                     )
#                     max_val = 0
#                     min_val = 0
#             elif f == "litter":
#                 if -1 in sb40_ftype:
#                     max_val = np.max(sb40_rhof[sb40_ftype == -1])
#                     litter_arr = sb40_rhof[sb40_ftype == -1]
#                     min_val = np.min(litter_arr[litter_arr > 0])
#                 else:
#                     print(
#                         "WARNING: litter fuel not present in sb40. Continuing with grass only."
#                     )
#                     max_val = 0
#                     min_val = 0
#             else:
#                 max_val = np.max(sb40_rhof)
#                 min_val = np.min(sb40_rhof[sb40_rhof > 0])
#             calibrated[f] = self._maxmin_calibration(
#                 self.duet_dict[f], max_val, min_val
#             )
#         self.calibrated_array = self._combine_fuel_types(calibrated_dict=calibrated)
#         self.calibrated = True
#         self.calibrated_fuel_type.append(fuel_type)
#         self.calibrated_fuel_type = self._flatten(self.calibrated_fuel_type)
#         self.calibration_method.append("sb40")
#         self.duet_dict = self._get_input_array()


#     def _query_landfire(self, delete_files: bool = True) -> np.ndarray:
#         """
#         Download a grid of SB40 fuel models from Landfire for the unit and convert to a numpy array

#         Parameters
#         ----------
#         delete_files: bool = True
#             Whether to delete intermediate tif files. Defaults to True

#         Returns
#         -------
#         NumPy Array
#             A numpy array of the SB40 FBFM keys for the site
#         """

#         # Name intermediate files
#         temp = [
#             "landfire_sb40.zip",
#             "landfire_sb40.tif",
#             "sb40_upsampled.tif",
#             "sb40_cropped.tif",
#         ]

#         # Create a bounding box from fuelgrid zarr
#         coords = [
#             self.xmin,
#             self.ymin,
#             self.xmin,
#             self.ymax,
#             self.xmax,
#             self.ymax,
#             self.xmax,
#             self.ymin,
#             self.xmin,
#             self.ymin,
#         ]
#         poly = geojson.Polygon(coordinates=[coords], precision=8)
#         bbox = get_bbox_from_polygon(aoi_polygon=poly, crs=5070)

#         # Download Landfire data to output directory
#         lf = landfire.Landfire(bbox, output_crs="5070")
#         lf.request_data(
#             layers=["200F40_19"], output_path=Path(self.output_dir, "landfire_sb40.zip")
#         )

#         # Exctract tif from compressed download folder and rename
#         with zipfile.ZipFile(Path(self.output_dir, "landfire_sb40.zip")) as zf:
#             extension = ".tif"
#             rename = "landfire_sb40.tif"
#             info = zf.infolist()
#             for file in info:
#                 if file.filename.endswith(extension):
#                     file.filename = rename
#                     zf.extract(file, self.output_dir)

#         # Upsample landfire raster to the quicfire resolution
#         with rio.open(Path(self.output_dir, "landfire_sb40.tif")) as sb:
#             upscale_factor = 30 / self.dx  # lf res/qf res
#             profile = sb.profile.copy()
#             # resample data to target shape
#             data = sb.read(
#                 out_shape=(
#                     sb.count,
#                     int(sb.height * upscale_factor),
#                     int(sb.width * upscale_factor),
#                 ),
#                 resampling=Resampling.nearest,
#             )

#             # scale image transform
#             transform = sb.transform * sb.transform.scale(
#                 (sb.width / data.shape[-1]), (sb.height / data.shape[-2])
#             )
#             profile.update(
#                 {
#                     "height": data.shape[-2],
#                     "width": data.shape[-1],
#                     "transform": transform,
#                 }
#             )
#             with rio.open(
#                 Path(self.output_dir, "sb40_upsampled.tif"), "w", **profile
#             ) as dataset:
#                 dataset.write(data)

#         # Crop the upsampled raster to the unit bounds
#         with rio.open(Path(self.output_dir, "sb40_upsampled.tif"), "r+") as rst:
#             out_image, out_transform = rasterio.mask.mask(rst, [poly], crop=True)
#             out_meta = rst.meta
#             out_meta.update(
#                 {
#                     "driver": "GTiff",
#                     "height": out_image.shape[1],
#                     "width": out_image.shape[2],
#                     "transform": out_transform,
#                 }
#             )
#             with rio.open(
#                 Path(self.output_dir, "sb40_cropped.tif"), "w", **out_meta
#             ) as cropped:
#                 cropped.write(out_image)

#         # Read in the cropped raster as a numpy array
#         with rio.open(Path(self.output_dir, "sb40_cropped.tif")) as rst:
#             arr = rst.read(1)

#         if delete_files:
#             [
#                 Path(self.output_dir, file).unlink()
#                 for file in temp
#                 if Path(self.output_dir, file).exists()
#             ]

#         return arr[arr > 0]

#     # TODO: fix rasterio cropping issue (grr) so that landfire raster is same size as fuelgrid

#     def _get_sb40_fuel_params(self, params: pd.DataFrame = None) -> dict:
#         """
#         Builds a dictionary of SB40 fuel parameter values and converts them to
#         the official FastFuels units

#         Returns:
#             dict: SB40 parameters for each fuel model
#         """
#         # Load in the SB40 fuel parameters
#         if params is not None:
#             sb40_df = params.copy()
#         else:
#             fpath = Path("src", "data_module", "data", "sb40_parameters.csv")
#             with open(fpath) as fin:
#                 sb40_df = pd.read_csv(fin)

#         # Convert tons/ac-ft to kg/m^3
#         sb40_df["1_hr_kg_per_m3"] = sb40_df["1_hr_t_per_ac"] * 0.22417
#         sb40_df["10_hr_kg_per_m3"] = sb40_df["10_hr_t_per_ac"] * 0.22417
#         sb40_df["100_hr_kg_per_m3"] = sb40_df["100_hr_t_per_ac"] * 0.22417
#         sb40_df["live_herb_kg_per_m3"] = sb40_df["live_herb_t_per_ac"] * 0.22417
#         sb40_df["live_woody_kg_per_m3"] = sb40_df["live_woody_t_per_ac"] * 0.22417

#         # Convert inverse feet to meters
#         sb40_df["dead_1_hr_sav_ratio_1_per_m"] = (
#             sb40_df["dead_1_hr_sav_ratio_1_per_ft"] * 3.2808
#         )
#         sb40_df["live_herb_sav_ratio_1_per_m"] = (
#             sb40_df["live_herb_sav_ratio_1_per_ft"] * 3.2808
#         )
#         sb40_df["live_wood_sav_ratio_1_per_m"] = (
#             sb40_df["live_wood_sav_ratio_1_per_ft"] * 3.2808
#         )

#         # Convert percent to ratio
#         sb40_df["dead_fuel_extinction_moisture"] /= 100

#         # Convert feet to meters
#         sb40_df["fuel_bed_depth_m"] = sb40_df["fuel_bed_depth_ft"] * 0.3048

#         # Compute wet loading
#         sb40_df["wet_load"] = sb40_df["1_hr_kg_per_m3"] + sb40_df["live_herb_kg_per_m3"]

#         # Compute a live herb curing factor alpha as a function of wet loading.
#         # This is kind of a B.S. approach raised by Rod on a phone call with
#         # Anthony on 02/28/2023. I don't like this at all, but it is a temporary
#         # Fix for the BP3D team to run some simulations.
#         # low_load_fuel_models = [
#         sb40_df["alpha"] = [0.5 if rho > 1 else 1.0 for rho in sb40_df["wet_load"]]

#         # Compute dry loading
#         sb40_df["dry_herb_load"] = sb40_df["live_herb_kg_per_m3"] * sb40_df["alpha"]
#         sb40_df["dry_load"] = sb40_df["1_hr_kg_per_m3"] + sb40_df["dry_herb_load"]

#         # Compute SAV
#         sb40_df["sav_1hr_ratio"] = sb40_df["1_hr_kg_per_m3"] / sb40_df["dry_load"]
#         sb40_df["sav_1hr"] = (
#             sb40_df["sav_1hr_ratio"] * sb40_df["dead_1_hr_sav_ratio_1_per_m"]
#         )
#         sb40_df["sav_herb_ratio"] = sb40_df["dry_herb_load"] / sb40_df["dry_load"]
#         sb40_df["sav_herb"] = (
#             sb40_df["sav_herb_ratio"] * sb40_df["live_herb_sav_ratio_1_per_m"]
#         )
#         sb40_df["sav"] = sb40_df["sav_1hr"] + sb40_df["sav_herb"]

#         # Convert nan to 0
#         sb40_df.fillna(0, inplace=True)

#         ## NIKO ADDITIONS
#         # Create dictionary for assigning fuel types for DUET calibration
#         duet_dict = {
#             "NB": 0,  # 0 = NEUTRAL, i.e. not predominantly grass or litter
#             "GR": 1,  # 1 = GRASS predominantly
#             "GS": 1,
#             "SH": 1,  # I am considering shrubs as grass
#             "TU": 0,
#             "TL": -1,  # -1 = LITTER predominantly
#             "SB": 0,
#         }

#         # Add column to df with DUET designations
#         pattern = r"[0-9]"  # take out numbers from fbfm_type strings
#         sb40_df["fbfm_cat"] = sb40_df["fbfm_code"].apply(
#             lambda x: re.sub(pattern, "", x)
#         )
#         sb40_df["duet_fuel_type"] = sb40_df["fbfm_cat"].apply(
#             lambda x: duet_dict.get(x)
#         )
#         ## END NIKO ADDITIONS

#         # Build the dictionary with fuel parameters for the Scott and Burgan 40
#         # fire behavior fuel models. Dict format: key ->
#         # [name, loading (tons/ac), SAV (1/ft), ext. MC (percent), bed depth (ft)]
#         # Note: Eventually we want to get rid of this and just use the dataframe.
#         # This is legacy from the old parameter table json.
#         sb40_dict = {}
#         for key in sb40_df["key"]:
#             row = sb40_df[sb40_df["key"] == key]
#             sb40_dict[key] = [
#                 row["fbfm_code"].values[0],
#                 row["dry_load"].values[0],
#                 row["sav"].values[0],
#                 row["dead_fuel_extinction_moisture"].values[0],
#                 row["fuel_bed_depth_m"].values[0],
#                 row["duet_fuel_type"].values[0],
#             ]

#         return sb40_dict

#     def _get_sb40_arrays(self, sb40_keys: np.ndarray, sb40_dict: dict) -> tuple:
#         """
#         Use a dictionary of bulk density and fuel types that correspond to SB40
#         fuel models to assign those values across the study area.

#         Fuel types are as follows:
#         - 1: Predominantly grass. All cells with a GR, GS, or SH designation from SB40.
#         - -1: Predominantly tree litter. All cells with a TL designation from SB40.
#         - 0: Neither predominantly grass or tree litter. All other SB40 designations.

#         Returns:
#             - np.ndarray of fuel types
#             - np.ndarray of bulk density values as calculated by fastfuels
#         """
#         ftype_dict = {key: val[5] for key, val in sb40_dict.items()}
#         ftype_arr = np.vectorize(ftype_dict.get)(sb40_keys)

#         rhof_dict = {key: val[1] for key, val in sb40_dict.items()}
#         rhof_arr = np.vectorize(rhof_dict.get)(sb40_keys)

#         return ftype_arr, rhof_arr

#     def _combine_fuel_types(self, calibrated_dict) -> np.ndarray:
#         calibrated_duet = np.zeros((2, self.ny, self.nx))
#         if len(calibrated_dict.keys()) == 1:
#             ftype = list(calibrated_dict.keys())[0]
#             if ftype == "grass":
#                 calibrated_duet[0, :, :] = calibrated_dict["grass"]
#                 calibrated_duet[1, :, :] = self.duet_dict["litter"]
#             elif ftype == "litter":
#                 calibrated_duet[0, :, :] = self.duet_dict["grass"]
#                 calibrated_duet[1, :, :] = calibrated_dict["litter"]
#             else:
#                 grass_weights = self.duet_dict["grass"] / self.duet_dict["total"]
#                 litter_weights = self.duet_dict["litter"] / self.duet_dict["total"]
#                 calibrated_duet[0, :, :][
#                     np.where(self.original_duet_array[0, :, :] > 0)
#                 ] = (calibrated_dict["total"] * grass_weights)[
#                     np.where(self.original_duet_array[0, :, :] > 0)
#                 ]
#                 calibrated_duet[1, :, :][
#                     np.where(self.original_duet_array[0, :, :] > 0)
#                 ] = (calibrated_dict["total"] * litter_weights)[
#                     np.where(self.original_duet_array[0, :, :] > 0)
#                 ]
#         else:
#             calibrated_duet[0, :, :] = calibrated_dict["grass"]
#             calibrated_duet[1, :, :] = calibrated_dict["litter"]
#         return calibrated_duet


#     def _flatten(self, A):
#         rt = []
#         for i in A:
#             if isinstance(i, list):
#                 rt.extend(self._flatten(i))
#             else:
#                 rt.append(i)
#         return rt
